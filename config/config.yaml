api_europepmc_params:
  rest_articles:
    root_url: https://www.ebi.ac.uk/europepmc/webservices/rest/
  oai_service:
    root_url: http://europepmc.org/oai.cgi 
  annotations_api:
    root_url: http://www.ebi.ac.uk/europepmc/annotations_api/annotationsByArticleIds
  # Should only contains the Open access articles but didn't check it
  archive_api:
    root_url: https://europepmc.org/ftp/oa/pmcid.txt.gz
  archive_file: ./data/pmcid.txt.gz
  article_query_folder: ./data/query_articles/
  article_human_folder: ./data/articles_human/
  pmcid_species_file: ./data/pmcid_species.txt
  # Used in previous script to be able to rerun the script without redl everything. Maybe still needed
  # For the new downloading script
  pmcid_human_file: ./data/pmcid_humans.txt
  db_info_articles: ./data/db_info_articles.sqlite3
  record_files: true
  xml_origin: api # can be either 'api' if connect to the api or 'files' if use dl files

search_params:
  rerun_archive: true
  query:  eLIfe AND (HAS_FT:Y AND OPEN_ACCESS:Y )
  # found using the script species_exploration.ipynb for manual exploration of the different terms
  human_terms: [human, humans, man, homo sapiens, h. sapiens, homo_sapiens]

processing_params:
  sentence_location: ./data/candidate_sentences.csv
  json_human_folder: ./data/json_humans/
  token_sentences: [man, woman, male, female, men, women, males, females, sample]
  list_article_location: ./data/exploration/list_article.csv
  list_level_location: ./data/exploration/list_levels.csv
  list_tag_location: ./data/explorationlist_tags.csv
  list_attr_val_location: ./data/exploration/list_attr_val.csv
  list_sections_location: ./data/exploration/list_sections_details.csv
  list_parent_location: ./data/exploration/list_parent_details.csv

db_params:
  table_status: status
  table_sections: sections
  table_metadata: article_metadata
 
llm_params:
  model: "./llm_models/mistral_7b_instruct_v03"
  # model: "./llm_models/mistral_7b_v03"
  # adapter: "./llm_models/dev_mistral_7b_lora"
  # model: "./llm_models/mixtral_8x7b_v01"
  # instruct_model: True
  instruct_model: False
  training_set_path: ./data/training_data/final_dataset.hf
  model_outdir: ./llm_models/
  model_train_name: dev_
  bits_and_bytes_config:
    load_in_4bit: True
    bnb_4bit_quant_type: nf4
    bnb_4bit_use_double_quant: True
    # FIXME Not loaded (no torch no attribute, don't know why)
    # So I directly hardcode it into the llm_inference and llm_peft_training_code
    bnb_4bit_compute_dtype: torch.bfloat16
    
  generation_params:
    # context_size: 2048
    # n_threads: 10
    # n_gpu_layers: 8
    temperature: 0.1
    do_sample: True
    top_p: 0.95
    top_k: 40
    max_new_tokens: 5000
    repetition_penalty: 1.1
  lora_config: 
    r: 16
    target_modules: ["q_proj", "v_proj"]
    task_type: CAUSAL_LM
    lora_alpha: 32
    lora_dropout: 0.05
  trainer_params:
    bf16: True
    gradient_checkpointing: True
    learning_rate: 2.0e-5  # Get the .0 or the number is parsed as string
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    num_train_epochs: 3
    weight_decay: 0.01
    evaluation_strategy: epoch
    save_strategy: epoch
    # load_best_model_at_the_end: True
    push_to_hub: False
    # deepspeed: ./config/deepspeed.json

